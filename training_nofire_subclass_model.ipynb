{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6348404,"sourceType":"datasetVersion","datasetId":3655844},{"sourceId":7615850,"sourceType":"datasetVersion","datasetId":4427634}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\n\n# Define the directory paths\ntrain_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/train\"\nval_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/val\"\ntest_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/test\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Define a custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.folder_path = folder_path\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        label_mapping = {\n            \"Smoke_confounding_elements\": 0,\n            \"Forested_areas_without_confounding_elements\": 1,\n            \"Fire_confounding_elements\": 2\n#             \"Smoke_from_fires\": 3,\n#             \"Both_smoke_and_fire\": 4,\n        }\n\n\n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n            current_path = f\"{folder_path}/{class_label}\"\n            if class_label == \"nofire\":\n                for subclass_label in [\"Forested_areas_without_confounding_elements\",\n                                       \"Fire_confounding_elements\",\n                                       \"Smoke_confounding_elements\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[subclass_label]] * len(image_files))\n            else:\n                continue\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensure that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    \n    def __len__(self):\n        return len(self.image_files)\n\n# Create dataset instances\ntrain_dataset = WildfireDataset(train_path, feature_extractor)\n# val_dataset = WildfireDataset(val_path, feature_extractor)\ntest_dataset = WildfireDataset(test_path, feature_extractor)\n\n# Create DataLoader instances\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T12:26:06.492856Z","iopub.execute_input":"2024-02-13T12:26:06.493308Z","iopub.status.idle":"2024-02-13T12:26:36.674846Z","shell.execute_reply.started":"2024-02-13T12:26:06.493270Z","shell.execute_reply":"2024-02-13T12:26:36.673863Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-13 12:26:21.756794: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-13 12:26:21.756919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-13 12:26:22.028474: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ba383c731f40da93c86ed96b304fcb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the length of the subset dataset\nprint(\"Number of samples in the subset dataset:\", len(train_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n#     model = torch.nn.DataParallel(model)\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:49:08.581860Z","iopub.execute_input":"2024-02-11T08:49:08.582230Z","iopub.status.idle":"2024-02-11T08:49:10.350299Z","shell.execute_reply.started":"2024-02-11T08:49:08.582199Z","shell.execute_reply":"2024-02-11T08:49:10.349311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',num_labels=3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n    \n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Define training parameters\nnum_epochs = 5\naccuracies = []\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_losses = []\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        logits = outputs.logits\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss for the epoch\n    avg_train_loss = sum(train_losses) / len(train_losses)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/nofire_subclass_model_VIT/nofire_subclass_model_VIT',num_labels=3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T12:27:26.492297Z","iopub.execute_input":"2024-02-13T12:27:26.493006Z","iopub.status.idle":"2024-02-13T12:27:29.851473Z","shell.execute_reply.started":"2024-02-13T12:27:26.492973Z","shell.execute_reply":"2024-02-13T12:27:29.850558Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n# Initialize val_accuracies list\nval_accuracies = []\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\n# Store validation accuracy for further analysis\nval_accuracies.append(val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T12:27:32.465249Z","iopub.execute_input":"2024-02-13T12:27:32.466147Z","iopub.status.idle":"2024-02-13T12:30:06.305580Z","shell.execute_reply.started":"2024-02-13T12:27:32.466103Z","shell.execute_reply":"2024-02-13T12:30:06.304607Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 89.24%\nValidation Loss: 0.3459\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"nofire_subclass_model_VIT\")\nelse:\n    model.save_pretrained(\"nofire_subclass_model_VIT\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel.save_pretrained(\"nofire_subclass_model_VIT\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:22:03.007404Z","iopub.execute_input":"2024-02-11T10:22:03.008080Z","iopub.status.idle":"2024-02-11T10:22:03.594114Z","shell.execute_reply.started":"2024-02-11T10:22:03.008044Z","shell.execute_reply":"2024-02-11T10:22:03.593049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r nofire_subclass_model_VIT.zip nofire_subclass_model_VIT","metadata":{"execution":{"iopub.status.busy":"2024-02-11T10:22:23.168169Z","iopub.execute_input":"2024-02-11T10:22:23.168536Z","iopub.status.idle":"2024-02-11T10:22:42.720171Z","shell.execute_reply.started":"2024-02-11T10:22:23.168507Z","shell.execute_reply":"2024-02-11T10:22:42.719160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**nofire_subclass_model_VIT_10epochs**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained(\"/kaggle/input/capstone-models/nofire_subclass_model_VIT/nofire_subclass_model_VIT\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()  # Cross-entropy loss handles labels starting from 0\n\n# Define training parameters\nnum_epochs = 5\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\n\n\n# Training loop\nstart_epoch = 6  # Start from the next epoch after the previous training\n\nfor epoch in range(start_epoch, start_epoch + num_epochs):\n    model.train()\n    train_losses = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n    # Print average training loss for the epoch4\n    avg_train_loss = sum(train_losses) / len(train_losses)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T17:28:37.424635Z","iopub.execute_input":"2024-02-11T17:28:37.425790Z","iopub.status.idle":"2024-02-11T18:22:02.463028Z","shell.execute_reply.started":"2024-02-11T17:28:37.425751Z","shell.execute_reply":"2024-02-11T18:22:02.461655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/nofire_subclass_model_VIT_10epochs/nofire_subclass_model_VIT_10epochs',num_labels=3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T12:30:20.429568Z","iopub.execute_input":"2024-02-13T12:30:20.430984Z","iopub.status.idle":"2024-02-13T12:30:25.960695Z","shell.execute_reply.started":"2024-02-13T12:30:20.430948Z","shell.execute_reply":"2024-02-13T12:30:25.959637Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n# Initialize val_accuracies list\nval_accuracies = []\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\n# Store validation accuracy for further analysis\nval_accuracies.append(val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T12:30:29.021684Z","iopub.execute_input":"2024-02-13T12:30:29.022777Z","iopub.status.idle":"2024-02-13T12:32:46.980273Z","shell.execute_reply.started":"2024-02-13T12:30:29.022715Z","shell.execute_reply":"2024-02-13T12:32:46.979170Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 92.43%\nValidation Loss: 0.2538\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nmodel.save_pretrained(\"nofire_subclass_model_VIT_10epochs\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T19:00:47.054620Z","iopub.execute_input":"2024-02-11T19:00:47.055022Z","iopub.status.idle":"2024-02-11T19:00:47.972436Z","shell.execute_reply.started":"2024-02-11T19:00:47.054990Z","shell.execute_reply":"2024-02-11T19:00:47.971275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r nofire_subclass_model_VIT_10epochs.zip nofire_subclass_model_VIT_10epochs","metadata":{"execution":{"iopub.status.busy":"2024-02-11T19:01:19.308891Z","iopub.execute_input":"2024-02-11T19:01:19.309265Z","iopub.status.idle":"2024-02-11T19:01:41.288900Z","shell.execute_reply.started":"2024-02-11T19:01:19.309235Z","shell.execute_reply":"2024-02-11T19:01:41.287643Z"},"trusted":true},"execution_count":null,"outputs":[]}]}