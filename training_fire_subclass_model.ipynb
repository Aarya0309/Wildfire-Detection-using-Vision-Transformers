{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6348404,"sourceType":"datasetVersion","datasetId":3655844},{"sourceId":7615850,"sourceType":"datasetVersion","datasetId":4427634}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\n\n# Define the directory paths\ntrain_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/train\"\n# val_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/val\"\ntest_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/test\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Define a custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.folder_path = folder_path\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        label_mapping = {\n#             \"Smoke_confounding_elements\": 0,\n#             \"Forested_areas_without_confounding_elements\": 1,\n#             \"Fire_confounding_elements\": 2,\n            \"Smoke_from_fires\": 0,\n            \"Both_smoke_and_fire\": 1,\n        }\n\n\n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n            current_path = f\"{folder_path}/{class_label}\"\n            if class_label == \"nofire\":\n                continue\n            else:\n                for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[subclass_label]] * len(image_files))\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensure that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    \n    def __len__(self):\n        return len(self.image_files)\n\n# Create dataset instances\ntrain_dataset = WildfireDataset(train_path, feature_extractor)\n# val_dataset = WildfireDataset(val_path, feature_extractor)\ntest_dataset = WildfireDataset(test_path, feature_extractor)\n\n# Create DataLoader instances\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T08:00:41.903146Z","iopub.execute_input":"2024-02-13T08:00:41.904252Z","iopub.status.idle":"2024-02-13T08:00:42.024071Z","shell.execute_reply.started":"2024-02-13T08:00:41.904219Z","shell.execute_reply":"2024-02-13T08:00:42.023189Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the length of the subset dataset\nprint(\"Number of samples in the subset dataset:\", len(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:10:17.337334Z","iopub.execute_input":"2024-02-11T08:10:17.337727Z","iopub.status.idle":"2024-02-11T08:10:17.343152Z","shell.execute_reply.started":"2024-02-11T08:10:17.337691Z","shell.execute_reply":"2024-02-11T08:10:17.341941Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of samples in the subset dataset: 730\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Define training parameters\nnum_epochs = 5\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\n\n\n# Store losses and accuracies\ntrain_losses = []\naccuracies = []\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = criterion(logits, labels)  # Compute the appropriate loss function here\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss and accuracy for the epoch\n    avg_train_loss = sum(train_losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Store accuracies for further analysis\nprint(\"Accuracies:\", accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:11:11.242289Z","iopub.execute_input":"2024-02-11T08:11:11.242640Z","iopub.status.idle":"2024-02-11T08:28:13.239095Z","shell.execute_reply.started":"2024-02-11T08:11:11.242613Z","shell.execute_reply":"2024-02-11T08:28:13.238051Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Let's use 2 GPUs!\nEpoch 1/5, Avg Train Loss: 0.6433, Accuracy: 0.6575\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Avg Train Loss: 0.5159, Accuracy: 0.8192\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Avg Train Loss: 0.4054, Accuracy: 0.8795\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Avg Train Loss: 0.3105, Accuracy: 0.9247\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Avg Train Loss: 0.2257, Accuracy: 0.9534\nAccuracies: [0.6575342465753424, 0.8191780821917808, 0.8794520547945206, 0.9246575342465754, 0.9534246575342465]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Initialize val_accuracies list\nval_accuracies = []\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\n# Store validation accuracy for further analysis\nval_accuracies.append(val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:29:53.244815Z","iopub.execute_input":"2024-02-11T08:29:53.245489Z","iopub.status.idle":"2024-02-11T08:30:42.979543Z","shell.execute_reply.started":"2024-02-11T08:29:53.245455Z","shell.execute_reply":"2024-02-11T08:30:42.978371Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Validation Accuracy: 87.42%\nValidation Loss: 0.3312\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"fire_subclass_model_VIT\")\nelse:\n    model.save_pretrained(\"fire_subclass_model_VIT\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:32:16.331660Z","iopub.execute_input":"2024-02-11T08:32:16.332055Z","iopub.status.idle":"2024-02-11T08:32:16.743267Z","shell.execute_reply.started":"2024-02-11T08:32:16.332026Z","shell.execute_reply":"2024-02-11T08:32:16.742396Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r fire_subclass_model_VIT.zip fire_subclass_model_VIT","metadata":{"execution":{"iopub.status.busy":"2024-02-11T08:32:39.771623Z","iopub.execute_input":"2024-02-11T08:32:39.772083Z","iopub.status.idle":"2024-02-11T08:32:59.429774Z","shell.execute_reply.started":"2024-02-11T08:32:39.772050Z","shell.execute_reply":"2024-02-11T08:32:59.428625Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"  adding: fire_subclass_model_VIT/ (stored 0%)\n  adding: fire_subclass_model_VIT/model.safetensors (deflated 7%)\n  adding: fire_subclass_model_VIT/config.json (deflated 46%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Training for 5 more epochs**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/fire_subclass_model_VIT/fire_subclass_model_VIT',num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Define training parameters\nnum_epochs = 5\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\nstart_epoch = 6\n\n# Store losses and accuracies\ntrain_losses = []\naccuracies = []\n\n# Training loop\nfor epoch in range(start_epoch, start_epoch + num_epochs):\n    model.train()\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = criterion(logits, labels)  # Compute the appropriate loss function here\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss and accuracy for the epoch\n    avg_train_loss = sum(train_losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Store accuracies for further analysis\nprint(\"Accuracies:\", accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T18:41:31.330049Z","iopub.execute_input":"2024-02-12T18:41:31.330435Z","iopub.status.idle":"2024-02-12T18:58:32.521598Z","shell.execute_reply.started":"2024-02-12T18:41:31.330408Z","shell.execute_reply":"2024-02-12T18:58:32.520648Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/5, Avg Train Loss: 0.1633, Accuracy: 0.9712\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/5, Avg Train Loss: 0.1111, Accuracy: 0.9863\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/5, Avg Train Loss: 0.0801, Accuracy: 0.9890\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/5, Avg Train Loss: 0.0602, Accuracy: 0.9932\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/5, Avg Train Loss: 0.0474, Accuracy: 0.9959\nAccuracies: [0.9712328767123287, 0.9863013698630136, 0.989041095890411, 0.9931506849315068, 0.9958904109589041]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T18:58:32.523521Z","iopub.execute_input":"2024-02-12T18:58:32.524192Z","iopub.status.idle":"2024-02-12T18:59:16.635802Z","shell.execute_reply.started":"2024-02-12T18:58:32.524163Z","shell.execute_reply":"2024-02-12T18:59:16.634788Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 86.79%\nValidation Loss: 0.3500\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"fire_subclass_model_VIT_10epochs\")\nelse:\n    model.save_pretrained(\"fire_subclass_model_VIT_10epochs\")","metadata":{"execution":{"iopub.status.busy":"2024-02-12T19:00:47.285409Z","iopub.execute_input":"2024-02-12T19:00:47.285769Z","iopub.status.idle":"2024-02-12T19:00:47.783650Z","shell.execute_reply.started":"2024-02-12T19:00:47.285741Z","shell.execute_reply":"2024-02-12T19:00:47.782796Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r fire_subclass_model_VIT_10epochs.zip fire_subclass_model_VIT_10epochs","metadata":{"execution":{"iopub.status.busy":"2024-02-12T19:01:06.017419Z","iopub.execute_input":"2024-02-12T19:01:06.017765Z","iopub.status.idle":"2024-02-12T19:01:25.035629Z","shell.execute_reply.started":"2024-02-12T19:01:06.017737Z","shell.execute_reply":"2024-02-12T19:01:25.034515Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"  adding: fire_subclass_model_VIT_10epochs/ (stored 0%)\n  adding: fire_subclass_model_VIT_10epochs/config.json (deflated 47%)\n  adding: fire_subclass_model_VIT_10epochs/model.safetensors (deflated 7%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Running for more epochs , even 1 resulst in accuracy going down**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/fire_subclass_model_VIT/fire_subclass_model_VIT',num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Define training parameters\nnum_epochs = 3\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\nstart_epoch = 6\n\n# Store losses and accuracies\ntrain_losses = []\naccuracies = []\n\n# Training loop\nfor epoch in range(start_epoch, start_epoch + num_epochs):\n    model.train()\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = criterion(logits, labels)  # Compute the appropriate loss function here\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss and accuracy for the epoch\n    avg_train_loss = sum(train_losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Store accuracies for further analysis\nprint(\"Accuracies:\", accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T19:01:57.755648Z","iopub.execute_input":"2024-02-12T19:01:57.756419Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/3, Avg Train Loss: 0.1713, Accuracy: 0.9671\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oversampling**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom collections import defaultdict\n\n# Define the directory paths\ntrain_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/train\"\n# val_path = \"/kaggle/input/validation-edited/val\"\ntest_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/test\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Define a custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        label_mapping = {\n            \"Smoke_from_fires\": 0,\n            \"Both_smoke_and_fire\": 1,\n        }\n\n        # Count the number of samples for each label\n        label_counts = defaultdict(int)\n        \n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n                current_path = f\"{folder_path}/{class_label}\"\n                if class_label == \"nofire\":\n                    continue\n                else:\n                    for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                        current_subclass_path = f\"{current_path}/{subclass_label}\"\n                        image_files = os.listdir(current_subclass_path)\n                        self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                        label = label_mapping[subclass_label]\n                        self.labels.extend([label] * len(image_files))\n                        label_counts[label] += len(image_files)\n            \n\n        # Calculate the maximum count of samples among all labels\n        max_count = max(label_counts.values())\n\n        # Oversample label 1 to match the count of label 0\n        for label, count in label_counts.items():\n            if label == 1:\n#                 oversample_factor = max_count // count\n                oversample_factor = 1\n                self.image_files.extend([img_path for img_path, lbl in zip(self.image_files, self.labels) if lbl == label] * oversample_factor)\n                self.labels.extend([label] * (count * oversample_factor))\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensure that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    def __len__(self):\n        return len(self.image_files)\n\n# Create dataset instances\ntrain_dataset = WildfireDataset(train_path, feature_extractor)\n# val_dataset = WildfireDataset(val_path, feature_extractor)\ntest_dataset = WildfireDataset(test_path, feature_extractor)\n\n# Create DataLoader instances\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:22:03.846522Z","iopub.execute_input":"2024-02-13T06:22:03.847418Z","iopub.status.idle":"2024-02-13T06:22:03.985594Z","shell.execute_reply.started":"2024-02-13T06:22:03.847376Z","shell.execute_reply":"2024-02-13T06:22:03.984910Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Print label distribution after oversampling\nprint(\"\\nLabel distribution in the oversampled dataset:\")\noversampled_label_counts = torch.bincount(torch.tensor(train_dataset.labels))\nfor label, count in enumerate(oversampled_label_counts):\n    print(f\"Label {label}: {count} samples\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:22:16.143367Z","iopub.execute_input":"2024-02-13T06:22:16.143742Z","iopub.status.idle":"2024-02-13T06:22:16.207610Z","shell.execute_reply.started":"2024-02-13T06:22:16.143689Z","shell.execute_reply":"2024-02-13T06:22:16.206765Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\nLabel distribution in the oversampled dataset:\nLabel 0: 461 samples\nLabel 1: 538 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Define training parameters\nnum_epochs = 5\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\n\n\n# Store losses and accuracies\ntrain_losses = []\naccuracies = []\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = criterion(logits, labels)  # Compute the appropriate loss function here\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss and accuracy for the epoch\n    avg_train_loss = sum(train_losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Store accuracies for further analysis\nprint(\"Accuracies:\", accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:22:27.948501Z","iopub.execute_input":"2024-02-13T06:22:27.949364Z","iopub.status.idle":"2024-02-13T06:46:49.891617Z","shell.execute_reply.started":"2024-02-13T06:22:27.949329Z","shell.execute_reply":"2024-02-13T06:46:49.890620Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a7208452e64b9c8bb58f16fce4305a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303125b17950484e95f6fcb9a454495b"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Avg Train Loss: 0.6135, Accuracy: 0.7247\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Avg Train Loss: 0.4512, Accuracy: 0.8559\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Avg Train Loss: 0.3129, Accuracy: 0.9269\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Avg Train Loss: 0.1906, Accuracy: 0.9700\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Avg Train Loss: 0.1177, Accuracy: 0.9820\nAccuracies: [0.7247247247247247, 0.8558558558558559, 0.9269269269269269, 0.96996996996997, 0.9819819819819819]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:47:01.133785Z","iopub.execute_input":"2024-02-13T06:47:01.134480Z","iopub.status.idle":"2024-02-13T06:48:10.303087Z","shell.execute_reply.started":"2024-02-13T06:47:01.134451Z","shell.execute_reply":"2024-02-13T06:48:10.302134Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 84.86%\nValidation Loss: 0.3464\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"fire_subclass_model_VIT_oversampled\")\nelse:\n    model.save_pretrained(\"fire_subclass_model_VIT_oversampled\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:50:44.810128Z","iopub.execute_input":"2024-02-13T06:50:44.810761Z","iopub.status.idle":"2024-02-13T06:50:45.317751Z","shell.execute_reply.started":"2024-02-13T06:50:44.810729Z","shell.execute_reply":"2024-02-13T06:50:45.316587Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r fire_subclass_model_VIT_oversampled.zip fire_subclass_model_VIT_oversampled","metadata":{"execution":{"iopub.status.busy":"2024-02-13T06:50:59.464383Z","iopub.execute_input":"2024-02-13T06:50:59.464845Z","iopub.status.idle":"2024-02-13T06:51:18.512443Z","shell.execute_reply.started":"2024-02-13T06:50:59.464812Z","shell.execute_reply":"2024-02-13T06:51:18.511432Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"  adding: fire_subclass_model_VIT_oversampled/ (stored 0%)\n  adding: fire_subclass_model_VIT_oversampled/model.safetensors (deflated 7%)\n  adding: fire_subclass_model_VIT_oversampled/config.json (deflated 46%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Data Augmentation**","metadata":{}},{"cell_type":"code","source":"pip install imgaug","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:25:00.612345Z","iopub.execute_input":"2024-02-13T07:25:00.612743Z","iopub.status.idle":"2024-02-13T07:25:14.656442Z","shell.execute_reply.started":"2024-02-13T07:25:00.612712Z","shell.execute_reply":"2024-02-13T07:25:14.655321Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: imgaug in /opt/conda/lib/python3.10/site-packages (0.4.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from imgaug) (1.16.0)\nRequirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.10/site-packages (from imgaug) (1.24.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from imgaug) (1.11.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from imgaug) (9.5.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from imgaug) (3.7.4)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.10/site-packages (from imgaug) (0.22.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from imgaug) (4.9.0.80)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.10/site-packages (from imgaug) (2.33.1)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.10/site-packages (from imgaug) (1.8.5.post1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug) (3.2.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->imgaug) (0.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->imgaug) (2.8.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import imgaug as ia\nfrom imgaug import augmenters as iaa\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:28:04.377759Z","iopub.execute_input":"2024-02-13T07:28:04.378109Z","iopub.status.idle":"2024-02-13T07:28:05.248527Z","shell.execute_reply.started":"2024-02-13T07:28:04.378082Z","shell.execute_reply":"2024-02-13T07:28:05.247579Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom collections import defaultdict\nimport random\n\n# Define the directory paths\ntrain_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/train\"\nval_path = \"/kaggle/input/validation-edited/val\"\ntest_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/test\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\nfrom torchvision.transforms import RandomHorizontalFlip, RandomRotation, RandomResizedCrop, ToTensor, Compose\n\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        self.label_mapping = {\n            \"Smoke_from_fires\": 0,\n            \"Both_smoke_and_fire\": 1,\n        }\n        self.data_transform = Compose([\n            RandomHorizontalFlip(),\n            RandomRotation(10),\n            RandomResizedCrop(224, scale=(0.8, 1.0)),\n            ToTensor(),\n        ])\n\n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n                current_path = f\"{folder_path}/{class_label}\"\n                if class_label == \"nofire\":\n                    continue\n                else:\n                    for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                        current_subclass_path = f\"{current_path}/{subclass_label}\"\n                        image_files = os.listdir(current_subclass_path)\n                        self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                        self.labels.extend([self.label_mapping[subclass_label]] * len(image_files))\n            \n        \n        # Determine the number of samples in class label 1\n        self.num_label_1_samples = self.labels.count(1)\n        # Determine the number of samples to be added for class label 1 after augmentation\n        self.num_augmented_label_1_samples = self.num_label_1_samples\n\n        self.feature_extractor = feature_extractor\n        # Define augmentation pipeline specifically for label 1 images\n        self.label_1_aug = iaa.Sequential([\n            iaa.Affine(rotate=(-30, 30)),  # Random rotation\n            iaa.Fliplr(0.5),  # Horizontal flip with 50% probability\n            iaa.GaussianBlur(sigma=(0, 0.5)),  # Random Gaussian blur\n        ])\n\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if label == 1:\n            # Retrieve the image again to create a duplicate with augmentation\n            image_duplicate = Image.open(img_path).convert(\"RGB\")\n            image_duplicate = self.feature_extractor(images=image_duplicate, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n            # Apply augmentation to the duplicate\n            image_duplicate = self.label_1_aug(images=[image_duplicate])[0]\n\n            # Return both the original and augmented images, effectively doubling\n            return {\n                \"pixel_values\": torch.stack([image, image_duplicate]),\n                \"labels\": torch.tensor([label, label]),\n            }\n        else:\n            image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n            return {\n                \"pixel_values\": image,\n                \"labels\": torch.tensor(label),\n            }\n        # If the index is within the original number of samples for class label 1,\n        # return the original image for class label 1\n#         if idx < self.num_label_1_samples:\n#             img_path = self.image_files[idx]\n#             label = self.labels[idx]\n#             image = Image.open(img_path).convert(\"RGB\")\n#             image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n#             return {\n#                 \"pixel_values\": image,\n#                 \"labels\": torch.tensor(label),\n#             }\n        \n#         # If the index is beyond the original number of samples for class label 1,\n#         # apply data augmentation to an image from class label 1\n#         augmented_idx = idx - self.num_label_1_samples\n#         original_label_1_indices = [i for i, label in enumerate(self.labels) if label == 1]\n#         original_image_idx = original_label_1_indices[augmented_idx % self.num_label_1_samples]\n#         img_path = self.image_files[original_image_idx]\n#         label = self.labels[original_image_idx]\n#         image = Image.open(img_path).convert(\"RGB\")\n#         augmented_image = self.data_transform(image)\n#         augmented_image = self.feature_extractor(images=augmented_image.unsqueeze(0), return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n#         return {\n#             \"pixel_values\": augmented_image,\n#             \"labels\": torch.tensor(label),\n#         }\n\n\n    def __len__(self):\n        return len(self.image_files) + self.num_augmented_label_1_samples\n\n\n# Define data augmentation transforms\ndata_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = WildfireDataset(train_path, feature_extractor)\n# val_dataset = WildfireDataset(val_path, feature_extractor)\ntest_dataset = WildfireDataset(test_path, feature_extractor)\n\n# Create DataLoader instances\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:28:06.853594Z","iopub.execute_input":"2024-02-13T07:28:06.854638Z","iopub.status.idle":"2024-02-13T07:28:06.933110Z","shell.execute_reply.started":"2024-02-13T07:28:06.854604Z","shell.execute_reply":"2024-02-13T07:28:06.932179Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print label distribution after oversampling\nprint(\"\\nLabel distribution in the oversampled dataset:\")\noversampled_label_counts = torch.bincount(torch.tensor(train_dataset.labels))\nfor label, count in enumerate(oversampled_label_counts):\n    print(f\"Label {label}: {count} samples\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:28:12.662981Z","iopub.execute_input":"2024-02-13T07:28:12.663357Z","iopub.status.idle":"2024-02-13T07:28:12.669673Z","shell.execute_reply.started":"2024-02-13T07:28:12.663329Z","shell.execute_reply":"2024-02-13T07:28:12.668740Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nLabel distribution in the oversampled dataset:\nLabel 0: 461 samples\nLabel 1: 269 samples\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Undersampling**","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport random\n\n# Define a custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor, undersample_label=0):\n        self.folder_path = folder_path\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        label_mapping = {\n            \"Smoke_from_fires\": 0,\n            \"Both_smoke_and_fire\": 1,\n        }\n        self.undersample_label = undersample_label\n\n        # Dictionary to store image files for each label\n        self.label_images = defaultdict(list)\n\n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n            current_path = f\"{folder_path}/{class_label}\"\n            if class_label == \"nofire\":\n                continue\n            else:\n                for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    label = label_mapping[subclass_label]\n                    self.label_images[label].extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n\n        # Determine the number of samples for the undersampled label\n        num_samples_undersampled = min(len(self.label_images[undersample_label]), len(self.label_images[1 - undersample_label]))\n\n        # Randomly select a subset of samples for the undersampled label\n        undersampled_indices = random.sample(range(len(self.label_images[undersample_label])), num_samples_undersampled)\n        undersampled_images = [self.label_images[undersample_label][idx] for idx in undersampled_indices]\n\n        # Combine the undersampled images with the other label's images\n        self.image_files = undersampled_images + self.label_images[1 - undersample_label]\n        self.labels = [undersample_label] * num_samples_undersampled + [1 - undersample_label] * len(self.label_images[1 - undersample_label])\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensure that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    def __len__(self):\n        return len(self.image_files)\n\n# Usage example:\ntrain_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/train\"\ntest_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/test\"\n\n# Create dataset instance with undersampling for label 0\ntrain_dataset = WildfireDataset(train_path, feature_extractor, undersample_label=0)\n\n# Create DataLoader instance\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:34:54.898800Z","iopub.execute_input":"2024-02-13T07:34:54.899150Z","iopub.status.idle":"2024-02-13T07:34:54.923972Z","shell.execute_reply.started":"2024-02-13T07:34:54.899124Z","shell.execute_reply":"2024-02-13T07:34:54.923163Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Print label distribution after oversampling\nprint(\"\\nLabel distribution in the oversampled dataset:\")\noversampled_label_counts = torch.bincount(torch.tensor(train_dataset.labels))\nfor label, count in enumerate(oversampled_label_counts):\n    print(f\"Label {label}: {count} samples\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:35:03.028724Z","iopub.execute_input":"2024-02-13T07:35:03.029269Z","iopub.status.idle":"2024-02-13T07:35:03.037641Z","shell.execute_reply.started":"2024-02-13T07:35:03.029222Z","shell.execute_reply":"2024-02-13T07:35:03.036539Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\nLabel distribution in the oversampled dataset:\nLabel 0: 269 samples\nLabel 1: 269 samples\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Define training parameters\nnum_epochs = 5\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\n\n\n# Store losses and accuracies\ntrain_losses = []\naccuracies = []\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = criterion(logits, labels)  # Compute the appropriate loss function here\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss and accuracy for the epoch\n    avg_train_loss = sum(train_losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Store accuracies for further analysis\nprint(\"Accuracies:\", accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T07:35:23.997553Z","iopub.execute_input":"2024-02-13T07:35:23.998381Z","iopub.status.idle":"2024-02-13T07:49:16.150586Z","shell.execute_reply.started":"2024-02-13T07:35:23.998349Z","shell.execute_reply":"2024-02-13T07:49:16.149610Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd41ca215ef40f0a69eb22276d565a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"284cf2cb8fd14db9b1ab4ab4a1819156"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Avg Train Loss: 0.6595, Accuracy: 0.6394\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Avg Train Loss: 0.5742, Accuracy: 0.8309\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Avg Train Loss: 0.4863, Accuracy: 0.8792\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Avg Train Loss: 0.4018, Accuracy: 0.8941\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Avg Train Loss: 0.3245, Accuracy: 0.9182\nAccuracies: [0.6394052044609665, 0.8308550185873605, 0.879182156133829, 0.8940520446096655, 0.9182156133828996]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T08:01:03.894685Z","iopub.execute_input":"2024-02-13T08:01:03.895068Z","iopub.status.idle":"2024-02-13T08:01:50.801987Z","shell.execute_reply.started":"2024-02-13T08:01:03.895036Z","shell.execute_reply":"2024-02-13T08:01:50.801043Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Validation Accuracy: 83.02%\nValidation Loss: 0.4345\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"fire_subclass_model_VIT_undersampled\")\nelse:\n    model.save_pretrained(\"fire_subclass_model_VIT_undersampled\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T08:03:24.464738Z","iopub.execute_input":"2024-02-13T08:03:24.465143Z","iopub.status.idle":"2024-02-13T08:03:24.941963Z","shell.execute_reply.started":"2024-02-13T08:03:24.465114Z","shell.execute_reply":"2024-02-13T08:03:24.940944Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r fire_subclass_model_VIT_undersampled.zip fire_subclass_model_VIT_undersampled","metadata":{"execution":{"iopub.status.busy":"2024-02-13T08:03:27.714085Z","iopub.execute_input":"2024-02-13T08:03:27.715000Z","iopub.status.idle":"2024-02-13T08:03:46.735872Z","shell.execute_reply.started":"2024-02-13T08:03:27.714965Z","shell.execute_reply":"2024-02-13T08:03:46.734884Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"  adding: fire_subclass_model_VIT_undersampled/ (stored 0%)\n  adding: fire_subclass_model_VIT_undersampled/config.json (deflated 46%)\n  adding: fire_subclass_model_VIT_undersampled/model.safetensors (deflated 7%)\n","output_type":"stream"}]}]}