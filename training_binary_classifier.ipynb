{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7863205,"sourceType":"datasetVersion","datasetId":3655844},{"sourceId":7875048,"sourceType":"datasetVersion","datasetId":4621293},{"sourceId":7885408,"sourceType":"datasetVersion","datasetId":4628851},{"sourceId":7903909,"sourceType":"datasetVersion","datasetId":4427634}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\n\n# Directory paths\ntrain_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/train\"\nval_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/val\"\ntest_path = \"/kaggle/input/the-wildfire-dataset/the_wildfire_dataset/the_wildfire_dataset/test\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.folder_path = folder_path\n        self.image_files = []\n        self.labels = []\n        label_mapping = {\n            \"nofire\": 0,\n            \"fire\": 1,\n        }\n\n        # Populating image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n            current_path = f\"{folder_path}/{class_label}\"\n            if class_label == \"nofire\":\n                for subclass_label in [\"Forested_areas_without_confounding_elements\",\n                                       \"Fire_confounding_elements\",\n                                       \"Smoke_confounding_elements\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[class_label]] * len(image_files))\n            else:\n                for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[class_label]] * len(image_files))\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensuring that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    \n    def __len__(self):\n        return len(self.image_files)\n\n\ntrain_dataset = WildfireDataset(train_path, feature_extractor)\nval_dataset = WildfireDataset(val_path, feature_extractor)\ntest_dataset = WildfireDataset(test_path, feature_extractor)\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T08:42:24.128020Z","iopub.execute_input":"2024-02-10T08:42:24.128413Z","iopub.status.idle":"2024-02-10T08:42:42.680128Z","shell.execute_reply.started":"2024-02-10T08:42:24.128374Z","shell.execute_reply":"2024-02-10T08:42:42.679194Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-10 08:42:31.917376: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-10 08:42:31.917469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-10 08:42:32.037335: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95384adb6c384f288f4b8609d5cd260e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Define training parameters\nnum_epochs = 5\n\n# Store losses and accuracies\ntrain_losses = []\naccuracies = []\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_labels = []\n    epoch_predictions = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = criterion(logits, labels)  # Compute the appropriate loss function here\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n\n        # Store predictions and labels for accuracy calculation\n        epoch_labels.extend(labels.cpu().numpy())\n        epoch_predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n\n    # Calculate accuracy\n    accuracy = accuracy_score(epoch_labels, epoch_predictions)\n    accuracies.append(accuracy)\n\n    # Print average training loss and accuracy for the epoch\n    avg_train_loss = sum(train_losses[-len(train_dataloader):]) / len(train_dataloader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n# Store accuracies for further analysis\nprint(\"Accuracies:\", accuracies)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T08:42:54.296097Z","iopub.execute_input":"2024-02-10T08:42:54.296735Z","iopub.status.idle":"2024-02-10T09:48:36.106169Z","shell.execute_reply.started":"2024-02-10T08:42:54.296703Z","shell.execute_reply":"2024-02-10T09:48:36.104918Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e5fdd58ab34a19bf563a7edd47f60d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7af2412bf404c57ba9436940924c346"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Avg Train Loss: 0.5513, Accuracy: 0.8203\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Avg Train Loss: 0.2510, Accuracy: 0.9470\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Avg Train Loss: 0.1237, Accuracy: 0.9762\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Avg Train Loss: 0.0742, Accuracy: 0.9894\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Avg Train Loss: 0.0516, Accuracy: 0.9926\nAccuracies: [0.8203497615262321, 0.9470058293587705, 0.9761526232114467, 0.9894011658717541, 0.9925808161102279]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Initialize val_accuracies list\nval_accuracies = []\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if np.any(np.isnan(outputs.loss.cpu().numpy())):\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Validation Loss: {val_loss:.4f}\")\n\n# Store validation accuracy for further analysis\nval_accuracies.append(val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T09:56:47.805497Z","iopub.execute_input":"2024-02-10T09:56:47.806439Z","iopub.status.idle":"2024-02-10T10:00:04.556173Z","shell.execute_reply.started":"2024-02-10T09:56:47.806404Z","shell.execute_reply":"2024-02-10T10:00:04.554974Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 97.80%\nValidation Loss: 0.0730\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"Binary_VIT\")\nelse:\n    model.save_pretrained(\"Binary_VIT\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T10:01:12.147386Z","iopub.execute_input":"2024-02-10T10:01:12.148091Z","iopub.status.idle":"2024-02-10T10:01:12.710972Z","shell.execute_reply.started":"2024-02-10T10:01:12.148056Z","shell.execute_reply":"2024-02-10T10:01:12.709043Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r binary_VIT.zip Binary_VIT","metadata":{"execution":{"iopub.status.busy":"2024-02-10T10:03:33.632305Z","iopub.execute_input":"2024-02-10T10:03:33.632941Z","iopub.status.idle":"2024-02-10T10:03:52.565909Z","shell.execute_reply.started":"2024-02-10T10:03:33.632909Z","shell.execute_reply":"2024-02-10T10:03:52.564874Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"  adding: Binary_VIT/ (stored 0%)\n  adding: Binary_VIT/model.safetensors (deflated 7%)\n  adding: Binary_VIT/config.json (deflated 46%)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\n\n# Directory paths\ntest_path = \"/kaggle/input/test-dataset/test - Copy\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.folder_path = folder_path\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        label_mapping = {\n            \"nofire\": 0,\n            \"fire\": 1,\n        }\n\n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n            current_path = f\"{folder_path}/{class_label}\"\n            if class_label == \"nofire\":\n                for subclass_label in [\"Forested_areas_without_confounding_elements\",\n                                       \"Fire_confounding_elements\",\n                                       \"Smoke_confounding_elements\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[class_label]] * len(image_files))\n            else:\n                for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[class_label]] * len(image_files))\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensure that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    \n    def __len__(self):\n        return len(self.image_files)\n\n# Create dataset instances\ntest_dataset = WildfireDataset(test_path, feature_extractor)\n\n# Create DataLoader instances\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:42:46.458162Z","iopub.execute_input":"2024-03-21T08:42:46.458535Z","iopub.status.idle":"2024-03-21T08:43:05.707833Z","shell.execute_reply.started":"2024-03-21T08:42:46.458498Z","shell.execute_reply":"2024-03-21T08:43:05.706790Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-21 08:42:54.906507: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-21 08:42:54.906618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-21 08:42:55.036581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffde10542a0d49ad8497144fc7cb8654"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, matthews_corrcoef\nimport torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/binary_VIT/Binary_VIT', num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Initialize val_accuracies list and other metrics lists\nval_accuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\nspecificities = []\nfnrs = []\nmccs = []\nroc_auc_scores = []\nconf_matrices = []\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if torch.isnan(outputs.loss).any():\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\n# Calculate precision, recall, F1-score, specificity, FNR, MCC, ROC-AUC, and confusion matrix\nprecision = precision_score(val_labels, val_preds)\nrecall = recall_score(val_labels, val_preds)\nf1 = f1_score(val_labels, val_preds)\nconf_matrix = confusion_matrix(val_labels, val_preds)\ntn, fp, fn, tp = conf_matrix.ravel()\nspecificity = tn / (tn + fp)\nfnr = fn / (fn + tp)\nmcc = matthews_corrcoef(val_labels, val_preds)\nroc_auc = roc_auc_score(val_labels, val_preds)\n\n# Print metrics\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"Specificity: {specificity:.4f}\")\nprint(f\"FNR: {fnr:.4f}\")\nprint(f\"MCC: {mcc:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n# Store metrics for further analysis\nval_accuracies.append(val_accuracy)\nprecisions.append(precision)\nrecalls.append(recall)\nf1_scores.append(f1)\nspecificities.append(specificity)\nfnrs.append(fnr)\nmccs.append(mcc)\nroc_auc_scores.append(roc_auc)\nconf_matrices.append(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T08:44:06.541214Z","iopub.execute_input":"2024-03-21T08:44:06.541879Z","iopub.status.idle":"2024-03-21T08:47:45.665996Z","shell.execute_reply.started":"2024-03-21T08:44:06.541845Z","shell.execute_reply":"2024-03-21T08:47:45.664955Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 97.80%\nPrecision: 0.9747\nRecall: 0.9686\nF1-score: 0.9716\nSpecificity: 0.9841\nFNR: 0.0314\nMCC: 0.9537\nROC-AUC: 0.9763\nConfusion Matrix:\n[[247   4]\n [  5 154]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\n\n# Directory paths\ntrain_path = \"/kaggle/input/train-original/train - Copy\"\n\n# ViT Feature Extractor\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n\n# Custom dataset class\nclass WildfireDataset(Dataset):\n    def __init__(self, folder_path, feature_extractor):\n        self.folder_path = folder_path\n        self.image_files = []  # List to store image file paths\n        self.labels = []  # List to store corresponding labels\n        label_mapping = {\n            \"nofire\": 0,\n            \"fire\": 1,\n        }\n\n        # Populate image_files and labels based on the clarified folder structure\n        for class_label in [\"nofire\", \"fire\"]:\n            current_path = f\"{folder_path}/{class_label}\"\n            if class_label == \"nofire\":\n                for subclass_label in [\"Forested_areas_without_confounding_elements\",\n                                       \"Fire_confounding_elements\",\n                                       \"Smoke_confounding_elements\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[class_label]] * len(image_files))\n            else:\n                for subclass_label in [\"Smoke_from_fires\", \"Both_smoke_and_fire\"]:\n                    current_subclass_path = f\"{current_path}/{subclass_label}\"\n                    image_files = os.listdir(current_subclass_path)\n                    self.image_files.extend([f\"{current_subclass_path}/{img}\" for img in image_files])\n                    self.labels.extend([label_mapping[class_label]] * len(image_files))\n\n        self.feature_extractor = feature_extractor\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        label = self.labels[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # Ensure that the image has the correct shape (num_channels, height, width)\n        image = self.feature_extractor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        return {\n            \"pixel_values\": image,\n            \"labels\": torch.tensor(label),\n        }\n\n    \n    def __len__(self):\n        return len(self.image_files)\n\n# Create dataset instances\ntrain_dataset = WildfireDataset(train_path, feature_extractor)\n\n# Create DataLoader instances\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T09:00:29.632452Z","iopub.execute_input":"2024-03-21T09:00:29.633601Z","iopub.status.idle":"2024-03-21T09:00:30.015425Z","shell.execute_reply.started":"2024-03-21T09:00:29.633567Z","shell.execute_reply":"2024-03-21T09:00:30.014292Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\n\n# Load the fine-tuned model\nmodel = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/binary_VIT/Binary_VIT', num_labels=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()  # Cross-entropy loss handles labels starting from 0\n\n# Define training parameters\nnum_epochs = 5\n\n# for param in model.base_model.parameters():\n#     param.requires_grad = False  # Freeze initial layers\n\n\n# Training loop\nstart_epoch = 6  # Start from the next epoch after the previous training\n\nfor epoch in range(start_epoch, start_epoch + num_epochs):\n    model.train()\n    train_losses = []\n\n    for batch in train_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=labels)\n        logits = outputs.logits\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n\n    # Print average training loss for the epoch4\n    avg_train_loss = sum(train_losses) / len(train_losses)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Avg Train Loss: {avg_train_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T09:00:40.891230Z","iopub.execute_input":"2024-03-21T09:00:40.892025Z","iopub.status.idle":"2024-03-21T10:05:23.317882Z","shell.execute_reply.started":"2024-03-21T09:00:40.891992Z","shell.execute_reply":"2024-03-21T10:05:23.316738Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/5, Avg Train Loss: 0.4021\nEpoch 8/5, Avg Train Loss: 0.2940\nEpoch 9/5, Avg Train Loss: 0.1285\nEpoch 10/5, Avg Train Loss: 0.0841\nEpoch 11/5, Avg Train Loss: 0.0530\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, matthews_corrcoef\nimport torch\nfrom transformers import ViTForImageClassification\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport torch.nn as nn\n\n# Define the loss function (criterion)\ncriterion = nn.CrossEntropyLoss()\n\n# model = ViTForImageClassification.from_pretrained('/kaggle/input/capstone-models/binary_VIT/Binary_VIT', num_labels=2)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# if torch.cuda.device_count() > 1:\n#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n#     model = torch.nn.DataParallel(model)\n# model.to(device)\n\n# Initialize val_accuracies list and other metrics lists\nval_accuracies = []\nprecisions = []\nrecalls = []\nf1_scores = []\nspecificities = []\nfnrs = []\nmccs = []\nroc_auc_scores = []\nconf_matrices = []\n\n# Validation loop\nmodel.eval()\nval_losses = []\nval_preds = []\nval_labels = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        inputs = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Check for non-empty batches\n        if inputs.size(0) == 0:\n            continue\n\n        outputs = model(inputs, labels=labels)\n        \n        # Check for NaN values in the loss\n        if torch.isnan(outputs.loss).any():\n            print(\"NaN loss encountered. Skipping batch.\")\n            continue\n\n        # Compute the loss using the appropriate criterion\n        loss = criterion(outputs.logits, labels)\n        val_losses.append(loss.item())\n\n        predictions = torch.argmax(outputs.logits, dim=1)\n        val_preds.extend(predictions.cpu().numpy())\n        val_labels.extend(labels.cpu().numpy())\n\n# Calculate validation accuracy\nval_accuracy = accuracy_score(val_labels, val_preds)\nval_loss = sum(val_losses) / len(val_losses)\n\n# Calculate precision, recall, F1-score, specificity, FNR, MCC, ROC-AUC, and confusion matrix\nprecision = precision_score(val_labels, val_preds)\nrecall = recall_score(val_labels, val_preds)\nf1 = f1_score(val_labels, val_preds)\nconf_matrix = confusion_matrix(val_labels, val_preds)\ntn, fp, fn, tp = conf_matrix.ravel()\nspecificity = tn / (tn + fp)\nfnr = fn / (fn + tp)\nmcc = matthews_corrcoef(val_labels, val_preds)\nroc_auc = roc_auc_score(val_labels, val_preds)\n\n# Print metrics\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"Specificity: {specificity:.4f}\")\nprint(f\"FNR: {fnr:.4f}\")\nprint(f\"MCC: {mcc:.4f}\")\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\n\n# Store metrics for further analysis\nval_accuracies.append(val_accuracy)\nprecisions.append(precision)\nrecalls.append(recall)\nf1_scores.append(f1)\nspecificities.append(specificity)\nfnrs.append(fnr)\nmccs.append(mcc)\nroc_auc_scores.append(roc_auc)\nconf_matrices.append(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T10:06:53.689847Z","iopub.execute_input":"2024-03-21T10:06:53.690638Z","iopub.status.idle":"2024-03-21T10:09:50.768483Z","shell.execute_reply.started":"2024-03-21T10:06:53.690606Z","shell.execute_reply":"2024-03-21T10:09:50.767436Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3176: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 97.80%\nPrecision: 0.9808\nRecall: 0.9623\nF1-score: 0.9714\nSpecificity: 0.9880\nFNR: 0.0377\nMCC: 0.9537\nROC-AUC: 0.9752\nConfusion Matrix:\n[[248   3]\n [  6 153]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the fine-tuned model\nif isinstance(model, torch.nn.DataParallel):\n    model.module.save_pretrained(\"Binary_VIT_10epochs\")\nelse:\n    model.save_pretrained(\"Binary_VIT_10epochs\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T10:11:31.144697Z","iopub.execute_input":"2024-03-21T10:11:31.145461Z","iopub.status.idle":"2024-03-21T10:11:31.585717Z","shell.execute_reply.started":"2024-03-21T10:11:31.145427Z","shell.execute_reply":"2024-03-21T10:11:31.584549Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create a zip archive of a folder\n!zip -r Binary_VIT_10epochs.zip Binary_VIT_10epochs","metadata":{"execution":{"iopub.status.busy":"2024-03-21T10:11:39.119869Z","iopub.execute_input":"2024-03-21T10:11:39.120247Z","iopub.status.idle":"2024-03-21T10:12:00.007895Z","shell.execute_reply.started":"2024-03-21T10:11:39.120218Z","shell.execute_reply":"2024-03-21T10:12:00.006679Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"  adding: Binary_VIT_10epochs/ (stored 0%)\n  adding: Binary_VIT_10epochs/model.safetensors (deflated 7%)\n  adding: Binary_VIT_10epochs/config.json (deflated 46%)\n","output_type":"stream"}]}]}